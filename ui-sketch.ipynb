{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":68289,"databundleVersionId":7586851,"sourceType":"competition"},{"sourceId":2445924,"sourceType":"datasetVersion","datasetId":1092122}],"dockerImageVersionId":30648,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-02-02T18:07:07.503988Z","iopub.execute_input":"2024-02-02T18:07:07.504377Z","iopub.status.idle":"2024-02-02T18:07:08.543205Z","shell.execute_reply.started":"2024-02-02T18:07:07.504345Z","shell.execute_reply":"2024-02-02T18:07:08.542132Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"# Imports for Deep Learning\nfrom keras.layers import Conv2D, Dense, Dropout, Flatten\nfrom keras.models import Sequential\nfrom keras.preprocessing.image import ImageDataGenerator\n\n\n\n# Imports to view data\nimport cv2\nfrom glob import glob\nfrom matplotlib import pyplot as plt\nfrom numpy import floor\nimport random","metadata":{"execution":{"iopub.status.busy":"2024-02-02T18:07:08.545270Z","iopub.execute_input":"2024-02-02T18:07:08.545768Z","iopub.status.idle":"2024-02-02T18:07:22.722378Z","shell.execute_reply.started":"2024-02-02T18:07:08.545735Z","shell.execute_reply":"2024-02-02T18:07:22.721299Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"2024-02-02 18:07:10.503346: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-02-02 18:07:10.503443: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-02-02 18:07:10.651333: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}]},{"cell_type":"code","source":"data_dir = \"/kaggle/input/uisketch\"\ntarget_size = (224, 224)\ntarget_dims = (224, 224, 3) # add channel for RGB\nn_classes = 21\nval_frac = 0.1\nbatch_size = 64\nfrom keras.preprocessing.image import ImageDataGenerator\n\n# Define the data augmentation parameters\ndata_augmentor = ImageDataGenerator(\n    samplewise_center=True,\n    samplewise_std_normalization=True,\n)\n\n# Generate augmented training data\ntrain_generator = data_augmentor.flow_from_directory(\n    data_dir,\n    target_size=target_size,\n    batch_size=batch_size,\n    shuffle=False,\n)\n\n","metadata":{"execution":{"iopub.status.busy":"2024-02-02T18:07:22.723663Z","iopub.execute_input":"2024-02-02T18:07:22.724231Z","iopub.status.idle":"2024-02-02T18:07:25.043734Z","shell.execute_reply.started":"2024-02-02T18:07:22.724203Z","shell.execute_reply":"2024-02-02T18:07:25.042822Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Found 19898 images belonging to 21 classes.\n","output_type":"stream"}]},{"cell_type":"code","source":"from tensorflow.keras.layers import Dense, Flatten, Conv2D, Dropout, GlobalAveragePooling2D\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.applications import InceptionV3\nfrom tensorflow.keras.applications import NASNetMobile\nfrom tensorflow.keras.applications import ResNet50\nfrom tensorflow.keras.layers import GlobalAveragePooling2D, Dense, Dropout\nfrom tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\nfrom tensorflow.keras.applications import Xception\n\n\ndef create_inceptionv3_model_with_callbacks(input_shape=(224, 224, 3), num_classes=21):\n    base_model = InceptionV3(weights='imagenet', include_top=False, input_shape=input_shape)\n    for layer in base_model.layers:\n        layer.trainable = False\n    \n    model = Sequential()\n    model.add(base_model)\n    model.add(GlobalAveragePooling2D())\n    model.add(Dense(256, activation='relu'))\n    model.add(Dropout(0.5))\n    model.add(Dense(num_classes, activation='softmax'))\n    \n    model.compile(loss='categorical_crossentropy',\n                  optimizer='adam',\n                  metrics=['accuracy'])\n    \n    return model\n\ndef create_xception_model_with_callbacks(input_shape=(224, 224, 3), num_classes=21):\n    base_model = Xception(weights='imagenet', include_top=False, input_shape=input_shape)\n    for layer in base_model.layers:\n        layer.trainable = False\n    \n    model = Sequential()\n    model.add(base_model)\n    model.add(GlobalAveragePooling2D())\n    model.add(Dense(256, activation='relu'))\n    model.add(Dropout(0.5))\n    model.add(Dense(num_classes, activation='softmax'))\n    \n    model.compile(loss='categorical_crossentropy',\n                  optimizer='adam',\n                  metrics=['accuracy'])\n    \n    return model\nfrom tensorflow.keras.applications import DenseNet201\n\nfrom tensorflow.keras.applications import MobileNetV2\nfrom tensorflow.keras.applications import VGG16\nfrom tensorflow.keras.layers import GlobalAveragePooling2D, Dense, Dropout\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n\ndef create_resnet_model(input_shape=(224, 224, 3), num_classes=21):\n    base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n    for layer in base_model.layers:\n        layer.trainable = False\n    \n    model = Sequential()\n    model.add(base_model)\n    model.add(GlobalAveragePooling2D())\n    model.add(Dense(256, activation='relu'))\n    model.add(Dense(num_classes, activation='softmax'))\n    \n    model.compile(loss='categorical_crossentropy',\n                  optimizer=Adam(learning_rate=0.001),\n                  metrics=['accuracy'])\n    \n    return model\n\ndef create_efficientnet_model(input_shape=(224, 224, 3), num_classes=21):\n    base_model = InceptionV3(weights='imagenet', include_top=False, input_shape=input_shape)\n    for layer in base_model.layers:\n        layer.trainable = False\n    \n    model = Sequential()\n    model.add(base_model)\n    model.add(GlobalAveragePooling2D())\n    model.add(Dense(256, activation='relu'))\n    model.add(Dropout(0.5))\n    model.add(Dense(num_classes, activation='softmax'))\n    \n    model.compile(loss='categorical_crossentropy',\n                  optimizer=Adam(learning_rate=0.001),\n                  metrics=['accuracy'])\n    \n    return model\n\ndef create_mobilenet_model(input_shape=(224, 224, 3), num_classes=21):\n    base_model = MobileNetV2(weights='imagenet', include_top=False, input_shape=input_shape)\n    for layer in base_model.layers:\n        layer.trainable = False\n    \n    model = Sequential()\n    model.add(base_model)\n    model.add(GlobalAveragePooling2D())\n    model.add(Dense(1024, activation='relu'))\n    model.add(Dropout(0.5))\n    model.add(Dense(num_classes, activation='softmax'))\n    \n    model.compile(loss='categorical_crossentropy',\n                  optimizer=Adam(learning_rate=0.001),\n                  metrics=['accuracy'])\n    \n    return model\n\ndef create_densenet_model(input_shape=(224, 224, 3), num_classes=21):\n    base_model = DenseNet201(weights='imagenet', include_top=False)\n    for layer in base_model.layers:\n        layer.trainable = False\n    \n    model = Sequential()\n    model.add(base_model)\n    model.add(Flatten()())\n    model.add(Dense(1024, activation='relu'))\n    model.add(Dropout(0.5))\n    model.add(Dense(num_classes, activation='softmax'))\n    \n    model.compile(loss='categorical_crossentropy',\n                  optimizer=Adam(learning_rate=0.001),\n                  metrics=['accuracy'])\n    \n    return model\nearly_stopping = EarlyStopping(monitor='accuracy', patience=5, restore_best_weights=True)\nreduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, min_lr=1e-6)\nmodel = create_resnet_model()\n\nsteps_per_epoch = train_generator.samples // train_generator.batch_size\n# Train your model\nhistory = model.fit(train_generator, steps_per_epoch=steps_per_epoch, epochs=30, callbacks=[early_stopping, reduce_lr])\nmodel.save(\"resnet152_model.h5\")\n","metadata":{"execution":{"iopub.status.busy":"2024-02-02T19:21:30.967115Z","iopub.execute_input":"2024-02-02T19:21:30.967994Z","iopub.status.idle":"2024-02-02T19:54:24.348781Z","shell.execute_reply.started":"2024-02-02T19:21:30.967959Z","shell.execute_reply":"2024-02-02T19:54:24.347531Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n94765736/94765736 [==============================] - 1s 0us/step\nEpoch 1/30\n310/310 [==============================] - 73s 215ms/step - loss: 2.4878 - accuracy: 0.2887 - lr: 0.0010\nEpoch 2/30\n310/310 [==============================] - 65s 210ms/step - loss: 1.6464 - accuracy: 0.4703 - lr: 0.0010\nEpoch 3/30\n310/310 [==============================] - 64s 208ms/step - loss: 1.5093 - accuracy: 0.5054 - lr: 0.0010\nEpoch 4/30\n310/310 [==============================] - 64s 207ms/step - loss: 1.3808 - accuracy: 0.5443 - lr: 0.0010\nEpoch 5/30\n310/310 [==============================] - 62s 200ms/step - loss: 1.3037 - accuracy: 0.5656 - lr: 0.0010\nEpoch 6/30\n310/310 [==============================] - 65s 210ms/step - loss: 1.2652 - accuracy: 0.5804 - lr: 0.0010\nEpoch 7/30\n310/310 [==============================] - 66s 213ms/step - loss: 1.2079 - accuracy: 0.5854 - lr: 0.0010\nEpoch 8/30\n310/310 [==============================] - 66s 214ms/step - loss: 1.1508 - accuracy: 0.6142 - lr: 0.0010\nEpoch 9/30\n310/310 [==============================] - 65s 209ms/step - loss: 1.1288 - accuracy: 0.6195 - lr: 0.0010\nEpoch 10/30\n310/310 [==============================] - 66s 213ms/step - loss: 1.0956 - accuracy: 0.6269 - lr: 0.0010\nEpoch 11/30\n310/310 [==============================] - 65s 209ms/step - loss: 1.0993 - accuracy: 0.6237 - lr: 0.0010\nEpoch 12/30\n310/310 [==============================] - 65s 209ms/step - loss: 1.0525 - accuracy: 0.6427 - lr: 0.0010\nEpoch 13/30\n310/310 [==============================] - 67s 214ms/step - loss: 1.0132 - accuracy: 0.6448 - lr: 0.0010\nEpoch 14/30\n310/310 [==============================] - 66s 212ms/step - loss: 0.9957 - accuracy: 0.6650 - lr: 0.0010\nEpoch 15/30\n310/310 [==============================] - 65s 209ms/step - loss: 0.9618 - accuracy: 0.6715 - lr: 0.0010\nEpoch 16/30\n310/310 [==============================] - 66s 213ms/step - loss: 0.9601 - accuracy: 0.6652 - lr: 0.0010\nEpoch 17/30\n310/310 [==============================] - 66s 213ms/step - loss: 0.9272 - accuracy: 0.6826 - lr: 0.0010\nEpoch 18/30\n310/310 [==============================] - 66s 212ms/step - loss: 0.9213 - accuracy: 0.6842 - lr: 0.0010\nEpoch 19/30\n310/310 [==============================] - 66s 213ms/step - loss: 0.8885 - accuracy: 0.6940 - lr: 0.0010\nEpoch 20/30\n310/310 [==============================] - 65s 209ms/step - loss: 0.8850 - accuracy: 0.6960 - lr: 0.0010\nEpoch 21/30\n310/310 [==============================] - 64s 207ms/step - loss: 0.8603 - accuracy: 0.7020 - lr: 0.0010\nEpoch 22/30\n310/310 [==============================] - 65s 211ms/step - loss: 0.8523 - accuracy: 0.7061 - lr: 0.0010\nEpoch 23/30\n310/310 [==============================] - 65s 210ms/step - loss: 0.8479 - accuracy: 0.7099 - lr: 0.0010\nEpoch 24/30\n310/310 [==============================] - 66s 211ms/step - loss: 0.8350 - accuracy: 0.7098 - lr: 0.0010\nEpoch 25/30\n310/310 [==============================] - 66s 212ms/step - loss: 0.7911 - accuracy: 0.7261 - lr: 0.0010\nEpoch 26/30\n310/310 [==============================] - 65s 210ms/step - loss: 0.8207 - accuracy: 0.7112 - lr: 0.0010\nEpoch 27/30\n310/310 [==============================] - 65s 208ms/step - loss: 0.7849 - accuracy: 0.7271 - lr: 0.0010\nEpoch 28/30\n310/310 [==============================] - 66s 212ms/step - loss: 0.7529 - accuracy: 0.7403 - lr: 0.0010\nEpoch 29/30\n310/310 [==============================] - 65s 210ms/step - loss: 0.7694 - accuracy: 0.7359 - lr: 0.0010\nEpoch 30/30\n310/310 [==============================] - 66s 212ms/step - loss: 0.7529 - accuracy: 0.7423 - lr: 0.0010\n","output_type":"stream"}]},{"cell_type":"code","source":"class_indices = train_generator.class_indices\nprint(\"Class indices:\", class_indices)\n\n# If you want to reverse the mapping for better readability\nlabel_to_class = {v: k for k, v in class_indices.items()}\nprint(\"Label to class mapping:\", label_to_class)","metadata":{"execution":{"iopub.status.busy":"2024-02-02T15:22:34.229486Z","iopub.status.idle":"2024-02-02T15:22:34.229819Z","shell.execute_reply.started":"2024-02-02T15:22:34.229658Z","shell.execute_reply":"2024-02-02T15:22:34.229671Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nfrom sklearn.metrics import confusion_matrix, classification_report\nimport numpy as np","metadata":{"execution":{"iopub.status.busy":"2024-02-02T15:22:34.231460Z","iopub.status.idle":"2024-02-02T15:22:34.231792Z","shell.execute_reply.started":"2024-02-02T15:22:34.231631Z","shell.execute_reply":"2024-02-02T15:22:34.231646Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#y_true = val_generator.classes\ny_pred_probs = model.predict_generator(val_generator)\ny_pred = np.argmax(y_pred_probs, axis=1)\n\n# Calculate confusion matrix\nconf_mat = confusion_matrix(y_true, y_pred)\n\n# Plot confusion matrix as a heatmap\nplt.figure(figsize=(10, 8))\nplt.imshow(conf_mat, cmap='viridis', interpolation='nearest')\nplt.colorbar()\nplt.title('Confusion Matrix')\nplt.xlabel('Predicted Label')\nplt.ylabel('True Label')\nplt.show()\n\n# Print classification report\nclass_names = list(label_to_class.values())\nprint(\"Classification Report:\")\nprint(classification_report(y_true, y_pred, target_names=class_names))\n","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2024-02-02T15:22:34.234004Z","iopub.status.idle":"2024-02-02T15:22:34.234460Z","shell.execute_reply.started":"2024-02-02T15:22:34.234226Z","shell.execute_reply":"2024-02-02T15:22:34.234245Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_true","metadata":{"execution":{"iopub.status.busy":"2024-02-02T15:22:34.235876Z","iopub.status.idle":"2024-02-02T15:22:34.236319Z","shell.execute_reply.started":"2024-02-02T15:22:34.236098Z","shell.execute_reply":"2024-02-02T15:22:34.236116Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred","metadata":{"execution":{"iopub.status.busy":"2024-02-02T15:22:34.237308Z","iopub.status.idle":"2024-02-02T15:22:34.237737Z","shell.execute_reply.started":"2024-02-02T15:22:34.237510Z","shell.execute_reply":"2024-02-02T15:22:34.237530Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n\ndata_dir = \"/kaggle/input/iasa-champ-24-ui-element-sketch-classification/ui_icons_dataset/ui_icons_dataset\"\ndata_augmentor = ImageDataGenerator(samplewise_center=True, \n                                    samplewise_std_normalization=True)\n\ntest_generator = data_augmentor.flow_from_directory(\n    data_dir,\n    target_size=target_size,\n    batch_size=batch_size,\n    shuffle=False,\n    classes=['test']\n)\nimage_names = test_generator.filenames","metadata":{"execution":{"iopub.status.busy":"2024-02-02T19:56:12.846736Z","iopub.execute_input":"2024-02-02T19:56:12.847153Z","iopub.status.idle":"2024-02-02T19:56:14.061425Z","shell.execute_reply.started":"2024-02-02T19:56:12.847118Z","shell.execute_reply":"2024-02-02T19:56:14.060549Z"},"trusted":true},"execution_count":30,"outputs":[{"name":"stdout","text":"Found 5143 images belonging to 1 classes.\n","output_type":"stream"}]},{"cell_type":"code","source":"y_pred = np.argmax(model.predict(test_generator), axis=1)\ny_pred","metadata":{"execution":{"iopub.status.busy":"2024-02-02T19:56:14.063553Z","iopub.execute_input":"2024-02-02T19:56:14.063875Z","iopub.status.idle":"2024-02-02T19:56:30.823121Z","shell.execute_reply.started":"2024-02-02T19:56:14.063846Z","shell.execute_reply":"2024-02-02T19:56:30.822108Z"},"trusted":true},"execution_count":31,"outputs":[{"name":"stdout","text":"81/81 [==============================] - 16s 202ms/step\n","output_type":"stream"},{"execution_count":31,"output_type":"execute_result","data":{"text/plain":"array([10, 10, 18, ..., 20, 11,  5])"},"metadata":{}}]},{"cell_type":"code","source":"out_mapping = {0: 'alert', 1: 'button', 2: 'card', 3: 'checkbox_checked', 4: 'checkbox_unchecked', 5: 'chip', 6: 'data_table', 7: 'dropdown_menu', 8: 'floating_action_button', 9: 'grid_list', 10: 'image', 11: 'label', 12: 'menu', 13: 'radio_button_checked', 14: 'radio_button_unchecked', 15: 'slider', 16: 'switch_disabled', 17: 'switch_enabled', 18: 'text_area', 19: 'text_field', 20: 'tooltip'}","metadata":{"execution":{"iopub.status.busy":"2024-02-02T19:56:30.824347Z","iopub.execute_input":"2024-02-02T19:56:30.824645Z","iopub.status.idle":"2024-02-02T19:56:30.830515Z","shell.execute_reply.started":"2024-02-02T19:56:30.824618Z","shell.execute_reply":"2024-02-02T19:56:30.829545Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"import pandas as pd","metadata":{"execution":{"iopub.status.busy":"2024-02-02T19:56:30.831832Z","iopub.execute_input":"2024-02-02T19:56:30.832188Z","iopub.status.idle":"2024-02-02T19:56:30.848142Z","shell.execute_reply.started":"2024-02-02T19:56:30.832146Z","shell.execute_reply":"2024-02-02T19:56:30.847207Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"print(out_mapping)\ny_pred = pd.Series(y_pred).map(out_mapping)\ny_pred","metadata":{"execution":{"iopub.status.busy":"2024-02-02T19:56:30.850580Z","iopub.execute_input":"2024-02-02T19:56:30.850884Z","iopub.status.idle":"2024-02-02T19:56:30.864015Z","shell.execute_reply.started":"2024-02-02T19:56:30.850858Z","shell.execute_reply":"2024-02-02T19:56:30.863053Z"},"trusted":true},"execution_count":34,"outputs":[{"name":"stdout","text":"{0: 'alert', 1: 'button', 2: 'card', 3: 'checkbox_checked', 4: 'checkbox_unchecked', 5: 'chip', 6: 'data_table', 7: 'dropdown_menu', 8: 'floating_action_button', 9: 'grid_list', 10: 'image', 11: 'label', 12: 'menu', 13: 'radio_button_checked', 14: 'radio_button_unchecked', 15: 'slider', 16: 'switch_disabled', 17: 'switch_enabled', 18: 'text_area', 19: 'text_field', 20: 'tooltip'}\n","output_type":"stream"},{"execution_count":34,"output_type":"execute_result","data":{"text/plain":"0                        image\n1                        image\n2                    text_area\n3              switch_disabled\n4                dropdown_menu\n                 ...          \n5138                 grid_list\n5139    floating_action_button\n5140                   tooltip\n5141                     label\n5142                      chip\nLength: 5143, dtype: object"},"metadata":{}}]},{"cell_type":"code","source":"import os\nimage_names = [os.path.basename(path) for path in image_names]\n","metadata":{"execution":{"iopub.status.busy":"2024-02-02T19:56:30.865104Z","iopub.execute_input":"2024-02-02T19:56:30.865423Z","iopub.status.idle":"2024-02-02T19:56:30.880626Z","shell.execute_reply.started":"2024-02-02T19:56:30.865391Z","shell.execute_reply":"2024-02-02T19:56:30.879668Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"submission = pd.DataFrame({\"ID\" : image_names,\"Target\" :y_pred}).to_csv(\"submission.csv\",index=False)","metadata":{"execution":{"iopub.status.busy":"2024-02-02T19:56:30.881968Z","iopub.execute_input":"2024-02-02T19:56:30.882511Z","iopub.status.idle":"2024-02-02T19:56:30.910766Z","shell.execute_reply.started":"2024-02-02T19:56:30.882476Z","shell.execute_reply":"2024-02-02T19:56:30.909709Z"},"trusted":true},"execution_count":36,"outputs":[]}]}